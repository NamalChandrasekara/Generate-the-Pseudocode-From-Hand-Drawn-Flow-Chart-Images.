# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V0gKeu7D4yHRCzspstcDTpi8ulYD65p0
"""

import os
import matplotlib.pyplot as plt
import keras_ocr
import cv2
import numpy as np
from google.colab import drive
drive.mount('/content/drive')

# Define the function to extract text and coordinates
def extract_text_and_coordinates(image_path):
    class TextClassifier:
        def __init__(self):
            # Initialize keras-ocr pipeline
            self.pipeline = keras_ocr.pipeline.Pipeline()

        def __set_image(self, image_path):
            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
            blur = cv2.GaussianBlur(image, (5, 5), 0)
            _, image = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
            return image

        def __get_bbox(self, image_path):
            images = keras_ocr.tools.read(image_path)
            self.image = images
            prediction_groups = self.pipeline.recognize([images])
            texts, results = [], []

            for ibox in prediction_groups[0]:
                box = ibox[1]
                texts.append(ibox[0])
                xs, ys = set(), set()
                for x in box:
                    xs.add(x[0])
                    ys.add(x[1])
                results.append(list(map(int, [min(xs), min(ys), max(xs), max(ys)]))) # ymin, xmin, ymax, xmax

            return results, texts

        def recognize(self, image_path):
            boxes, texts = self.__get_bbox(image_path)
            nodes = []

            for box, text in zip(boxes, texts):
                xmin, ymin, xmax, ymax = box
                nodes.append({'text': text, 'coordinate': [xmin, xmax, ymin, ymax]})

            return nodes

    # Initialize the classifier
    classifier = TextClassifier()

    # Recognize text in the image
    nodes = classifier.recognize(image_path)

    # Extract text and coordinates into separate arrays
    ex_text = [node['text'] for node in nodes]
    ex_co = [node['coordinate'] for node in nodes]

    return ex_text, ex_co

# Function to correct text using SymSpell
from symspellpy.symspellpy import SymSpell, Verbosity
import pkg_resources

# Initialize SymSpell object
sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)

# Load the dictionary
dictionary_path = pkg_resources.resource_filename(
    "symspellpy", "frequency_dictionary_en_82_765.txt")
sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)

def correct_text(text_array):
    corrected_text_array = []
    for text in text_array:
        suggestions = sym_spell.lookup(text, Verbosity.CLOSEST, max_edit_distance=2)
        if suggestions:
            corrected_text_array.append(suggestions[0].term)
        else:
            corrected_text_array.append(text)
    return corrected_text_array

# Example usage
if __name__ == "__main__":
    # Path to the image in Google Drive
    image_path = '/content/drive/MyDrive/Given_FlowCharts/System_Generated/4107.jpg'

    # Extract text and coordinates
    ex_text, ex_co = extract_text_and_coordinates(image_path)

    # Print the results
    print("Extracted Texts:", ex_text)
    print("Extracted Coordinates:", ex_co)

    # Optionally, display the image with bounding boxes
    image = cv2.imread(image_path)
    for coord in ex_co:
        xmin, xmax, ymin, ymax = coord
        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (255, 0, 0), 2)
        cv2.putText(image, ex_text[ex_co.index(coord)], (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)

    plt.figure(figsize=(10, 10))
    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    plt.axis('off')
    plt.show()

    # Correct the extracted text
    cr_text = correct_text(ex_text)

    print("Original words:", ex_text)
    print("Corrected words:", cr_text)